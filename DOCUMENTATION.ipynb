{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "823d7b7f",
   "metadata": {},
   "source": [
    "# DOCUMENTATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e927f0e",
   "metadata": {},
   "source": [
    "<h2>Introduction to SpokeLizard: A Dockerized Stream Analysis Pipeline</h2>\n",
    "\n",
    "In the realm of modern data analytics, real-time processing of streaming data presents both a challenge and an opportunity. \"SpokeLizard\" emerges as a sophisticated solution, embodying a dockerized pipeline designed to streamline the analysis of continuous data streams through a custom-built web application. This project represents a convergence of advanced technological frameworks aimed at enhancing the efficiency and scalability of data processing in dynamic environments.\n",
    "\n",
    "At its core, SpokeLizard harnesses Docker's containerization capabilities to encapsulate each component of the analytical pipeline, ensuring seamless deployment and portability across diverse computing environments. The web application, meticulously crafted for intuitive user interaction, serves as the gateway to this streamlined process, facilitating real-time data ingestion, processing, analysis, and visualization.\n",
    "\n",
    "Key functionalities of SpokeLizard include robust data aggregation mechanisms, scalable processing capabilities leveraging cloud-native technologies, and a responsive user interface tailored to empower analysts and stakeholders with actionable insights at unprecedented speeds. By leveraging Docker containers, SpokeLizard not only enhances deployment flexibility but also promotes modularity and maintainability, crucial for adapting to evolving data landscapes.\n",
    "\n",
    "In essence, SpokeLizard represents a pivotal advancement in stream analysis methodologies, poised to redefine how organizations harness the power of real-time data for informed decision-making. This project underscores a commitment to innovation, efficiency, and scalability in the domain of data analytics, promising transformative outcomes for industries embracing the era of continuous intelligence.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39675735",
   "metadata": {},
   "source": [
    "## THE PIPELINE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c88ce4",
   "metadata": {},
   "source": [
    "![Pipelinee](assets/pipeline.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a309fc6",
   "metadata": {},
   "source": [
    "I want to walk you through how we handle data at \"https://spokelizard-production.up.railway.app\", a site I built from scratch with my own custom APIs.\n",
    "\n",
    "It starts with Python scripts that interact directly with our APIs. These scripts are like keys unlocking a treasure trove of valuable data from our site.\n",
    "\n",
    "Once we've got the data, it flows through a slick pipeline, finely tuned like a ninja's sword. First up is Logstash, where it's transformed and primed for the next leg of its journey.\n",
    "\n",
    "Then it's onto Kafka, the beating heart of our system. Here, data moves fast and securely, ready to be analyzed by Spark. With machine learning algorithms at its core, Spark uncovers hidden insights in the data, distinguishing the good from the bad.\n",
    "\n",
    "The final results find their home in ElasticSearch, where we store them for future reference and analysis.\n",
    "\n",
    "And to make sense of it all, we use Kibana, turning raw data into interactive charts and dashboards."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14565ce3",
   "metadata": {},
   "source": [
    "<h2>Guide</h2>\n",
    "\n",
    "Here's a concise summary of using Docker Compose in English:\n",
    "\n",
    "1. **Configuration**: Docker Compose uses a YAML file (`docker-compose.yml`) to define the configuration of a multi-container application, including services, networks, and volumes.\n",
    "\n",
    "2. **Starting and Stopping**: To start all containers defined in the Compose file:\n",
    "   ```\n",
    "   docker-compose up\n",
    "   ```\n",
    "   To stop containers:\n",
    "   ```\n",
    "   docker-compose stop\n",
    "   ```\n",
    "   To remove containers, networks, and volumes:\n",
    "   ```\n",
    "   docker-compose down\n",
    "   ```\n",
    "\n",
    "3. **Service Management**: Each service can be configured with parameters such as Docker image, exposed ports, environment variables, and mounted volumes in the Compose file.\n",
    "\n",
    "4. **Multiple Environments**: Docker Compose supports environment variables to customize configuration across different environments (e.g., development, testing, production).\n",
    "\n",
    "5. **Container Communication**: It automates the creation of private networks for application containers, simplifying communication using container names instead of IP addresses.\n",
    "\n",
    "These points highlight how Docker Compose streamlines the development and management of multi-container Docker applications, providing a declarative and user-friendly environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7faf7ebf",
   "metadata": {},
   "source": [
    "<h3>Visualizing the data:</h3>\n",
    "\n",
    "To begin, open your web browser and navigate to http://localhost:5601/app/home.\n",
    "\n",
    "If a prompt appears, simply close it and proceed to click on \"Manage\". From the left-hand menu under Kibana, select \"Saved Objects\". Next, click on \"Import\" and choose the file named kibana_export.ndjson from your project folder.\n",
    "\n",
    "After confirming the import, navigate to the \"Main Dashboard\".\n",
    "\n",
    "Here, you'll find numerous graphs available, each representing data extracted from the web application.\n",
    "\n",
    "![Dashboard](assets/dashboard.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84bca093",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
